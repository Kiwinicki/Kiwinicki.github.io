"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/04/04/training-tricks","metadata":{"permalink":"/2025/04/04/training-tricks","source":"@site/blog/2025-04-04-training-tricks.md","title":"Random training tips and tricks","description":"List of all sort of things you can use make your training/finetuning go faster.  From one-line tricks to whole architecture changes. Order is random and grouping is somewhat arbitrary. Treat this as a knowledge dump.","date":"2025-04-04T00:00:00.000Z","tags":[],"readingTime":6.36,"hasTruncateMarker":true,"authors":[],"frontMatter":{},"unlisted":false,"nextItem":{"title":"GANs","permalink":"/2025/03/04/gans"}},"content":"List of all sort of things you can use make your training/finetuning go faster. \x3c!-- truncate --\x3e From one-line tricks to whole architecture changes. Order is random and grouping is somewhat arbitrary. Treat this as a knowledge dump.\\n\\n1. buy bigger GPU.\\n\\n## Optimizer\\n\\n- 8-bit Adam -  2\\\\*8-bit vs 2\\\\*32-bit = ~75% less memory for optimizer states\\n- Not storing activations from forward pass but recomputing them in backward pass (memory-compute tradeoff)\\n- \\"Stateless\\" optimizers - SGD, Lion. In contrast to e.g. Adam these don\'t store additional momentum variables (which takes additional 2x model size of space). The catch is these are not drop-in replacements and Adam was specially made for faster convergence & more stable training.  \\n- Optimizer states offloading to CPU, using [`torchao`](https://github.com/pytorch/ao/tree/main/torchao/optim#optimizer-cpu-offload):\\n    ```python\\n    optim = CPUOffloadOptimizer(model.parameters(), torch.optim.AdamW, fused=True)\\n    optim.load_state_dict(ckpt[\\"optim\\"])\\n    ```\\n\\n## PEFT\\n\\n- LoRA (Low-Rank Adaptation) - thin layer over frozen pretrained layers of your model. It uses a trick of decomposing a large matrix into two smaller low-rank ($n\\\\times m$ where $n << m$) matrices that gives huge memory savings. Slightly more formal:\\n\\n    $\\\\text{act}[(W + \\\\Delta W)\\\\cdot x + b]$, where:\\n    - $W=(d \\\\times k)$ pretrained weight tensor\\n    - $\\\\Delta W = AB$, (init LoRA tensors) where:\\n        - $A = (d\\\\times \\\\text{rank}) =\\\\mathcal{N}(0, \\\\sigma^2)$ at init\\n        - $B=(\\\\text{rank} \\\\times k)=0$ at init\\n        - at the begining $AB$ are no-op but thanks to $A$ being Gaussian there will be symmetry breaking\\n    - some good links about it: [Sebastian Raschka](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch), [AI Coffe Break](https://youtu.be/KEv-F5UkhxU?si=JcCjdEV-7tXPvk2r)\\n- QLoRA - It differs in that base model is quantized (usually to 4-bit) but LoRA layers are kept in 16-bit precision\\n- After training its good to merge LoRA layers with the base model for less overhead (but you can\'t swap them later).\\n- DoRA - decomposition of weight matrix into magnitude vector $m$ (euclidean distance) & directional matrix $V$ (angle) and train them separately.\\n- GaLore - LoRA but for gradient matrix. Supposedly works also for pretraining in contrast to LoRA, but I didn\'t tried it.\\n- Prefix Tuning - In place of the prompt you put a random init vector (the so-called prefix) and optimize it until you get the correct answer.\\n    - \\"+\\" tiny amount of parameters to tune\\n    - \\"-\\" takes context length (but alternatively you would put pre-prompt there)\\n    - \\"-\\" interpretability - these are not words, but you can decode it to \\"nearest\\" words but it often gibberish\\n\\n## Training initialization\\n\\n- init weights properly (relaying on default pytorch init isn\'t always optimal)\\n- LR scheduler (OneCycleLR, lr warmup, etc.) and lr search. Similar for BS (batch size warmup etc.)\\n- max out batch size to fill whole VRAM ([batch size finder](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.BatchSizeFinder.html))\\n- some rule regarding relation of LR & BS - [$LR \\\\approx \\\\sqrt{BS}$](https://x.com/cloneofsimo/status/1907731069878825400) (there is also issue of \\"[critical batch size](https://x.com/SeunghyunSEO7/status/1877188952920125836)\\")\\n- if you can\'t fit enough batch size for stable training (e.g. bsz=1) then use gradient accumulation. In pure pytorch it is calling opt.step() less frequently what results in effectively higher batch size.\\n- gradient clipping - prevents exploding gradients by capping their magnitude during backpropagation.\\n- Start from pretrained model (transfer learning) and swap last layer.\\n    - Freeze whole model except last layer and after few epochs gradually unfreeze rest of the layers.\\n\\n## Architecture\\n\\n- normalization layers (stabilize and speeding up training - you can use higher LR)\\n- turn off bias before BatchNorm (bn already does shift)\\n- MoE - model architecture which selectively activates only part of the model. memory-computation tradeoff. MoE faster achives same loss under the same computational budged compared to dense models.\\n- MoD (Mixture of Depths) - learned skip connection for each transformer block, model learns to not waste compute on easy tokens.\\n- Stochastic Depth - each layer in **deep** ConvNet have probability of not being dropped from 1.0 (for first layer) to 0.5 (for last layer). Simply dropout whole layers (prevents vanishing gradients, faster training,  better performance)\\n\\n### LLM only\\n\\n- SuperBPE - groups frequent word sequences into single tokens, improving efficiency and performance. Common word combinations get treated as one unit by the tokenizer, which reduces the number of easy-to-predict sequences. This creates a more balanced prediction difficulty across tokens, allowing the model to distribute computational effort more effectively. [Author explaination](https://x.com/alisawuffles/status/1903125390618661068)\\n\\n### Diffusion only\\n\\n- latent diffusion - diffuse in latent space, not pixel space. VAE encoder $\\\\rightarrow$ latent (diffuse $N\\\\times$) $\\\\rightarrow$ VAE decoder. SD1.5 VAE maybe big but you can use [TAESD](https://huggingface.co/madebyollin/taesd) ($(3\\\\cdot512\\\\cdot512)/(16\\\\cdot64\\\\cdot64)=12\\\\times$ compression, 5MB for enc/dec each and minimal computational overhead)\\n    - if you want train on ImageNet: https://huggingface.co/datasets/fal/cosmos-imagenet (compressed to 2.45GB)\\n- [Min-SNR](https://arxiv.org/abs/2303.09556) - method of adding a weightning to the loss based on the SNR (signal to noise ratio) of the timestep. It prevents conflicting gradients from different deniosing phases (beggining, mid and final refinements)\\n\\n## Mixed precision\\n\\nhttps://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html\\n\\nhttps://medium.com/@jbensnyder/solving-the-limits-of-mixed-precision-training-231019128b4b\\n\\nAll types other than FP32 are only faster on consumer cards due to Tensor cores which are only available on RTX cards. Tensor cores are used automatically when using mixed precision.\\n\\n- **FP32+FP16/BF16** (speed & memory*) - you store an extra copy of the model in 16-bit to be able to do faster calculations (~2x) (memory-compute trade-off). Gradients are also **computed** in 16-bit, but **stored** in FP32. BF16 should be more stable than FP16, as more range is more important for NNs than precision. BF16 stands from \\"Brain Float\\" from Google Brain btw.\\n    \\n    $$\\n    \\\\begin{align*}\\n    \\\\underbrace{(4+2\\\\ \\\\text{bytes})}_{\\\\text{{model FP32+FP16}}} + \\n    \\\\underbrace{(2\\\\ \\\\text{bytes})}_{\\\\text{{activation FP16}}} + \\\\underbrace{(4\\\\ \\\\text{bytes})}_{\\\\text{{gradients FP32}}} = 12\\\\ \\\\text{bytes (+8 bytes for Adam)}\\\\end{align*}\\n    $$\\n    \\n- turn on **TF32** (Tensor Float) (speed) - not all operations are supported in 16-bit mixed-precision and have to be done in FP32. Turning on TF32 replaces FP32 in computation (storage still in FP32) at speeds similar to FP16. TF32 is supported on Ampere arch and newer. Fun fact is that TF32 is 19-bit format but has \\"32\\" in name.\\n    \\n    ```python\\n    # The flag below controls whether to allow TF32 on matmul.\\n    # This flag defaults to False in PyTorch 1.12 and later.\\n    torch.backends.cuda.matmul.allow_tf32 = True\\n    \\n    # The flag below controls whether to allow TF32 on cuDNN. \\n    # This flag defaults to True.\\n    torch.backends.cudnn.allow_tf32 = True\\n    ```\\n    \\n- **FP32+FP8** (speed & memory) - there is possibility to do computations in 8-bit with FP32 accumulation but its more complicated to setup than AMP natively supported in PyTorch. Use [HF Accelerate](https://huggingface.co/docs/accelerate/usage_guides/low_precision_training) library to do this (from what I understand its a wrapper on 3 other packages [`TransformersEngine`, `MS-AMP`, and `torchao`] but not only this).\\n    \\n    $$\\n    \\\\begin{align*}\\n    \\\\underbrace{(1\\\\ \\\\text{byte})}_{\\\\text{{model FP8 E4M3}}} + \\n    \\\\underbrace{(1\\\\ \\\\text{byte})}_{\\\\text{{activation FP8 E4M3}}} + \\n    \\\\underbrace{(1\\\\ \\\\text{byte})}_{\\\\text{{gradients FP8 E5M2}}} = 3\\\\ \\\\text{bytes (+8 bytes for Adam)}\\\\end{align*}\\n    $$\\n\\n## Other\\n\\n- avoid moving tensors to another device `.to(device)`, create tensors directly on target device instead. If you don\'t have any synchronization later in code then you can use `.to(non_blocking=True)`\\n- use `torch.compile()` if it works, for me it usualy don\'t.\\n- set gradients to `None` instead of default `0` (but this can cause some unexpected behaviors - `None` isn\'t a number so operations with it produces `NaN`)\\n- use `.as_tensor()` rather than `.tensor()`. `torch.tensor()` always copies data. If you have a numpy array that you want to convert, use `torch.as_tensor()` or `torch.from_numpy()` to avoid copying the data.\\n- try \\"channel last\\" format for tensors and model (NCHW => NHWC), sometimes it\'s faster. [link](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)\\n- `torch.backends.cudnn.benchmark = True`\\n- slow dataloader optimizations [Simo tweet](https://x.com/cloneofsimo/status/1855608988681080910), [PyTorch forum](https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548/19)\\n- non-global Cross-Entropy calculation reduces memory usage spike at the end (especially beneficial for LLMs). [paper](https://arxiv.org/abs/2411.09009)\\n- checkpoint averaging - weighted average of previous checkpoints makes loss landscape more smooth and convex which speeds up training + reduces overfitting (applies to pretraining & finetuning)"},{"id":"/2025/03/04/gans","metadata":{"permalink":"/2025/03/04/gans","source":"@site/blog/2025-03-04-gans.md","title":"GANs","description":"Notes about GANs from vanilla GAN to CycleGAN and StyleGAN.","date":"2025-03-04T00:00:00.000Z","tags":[],"readingTime":8.415,"hasTruncateMarker":true,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Random training tips and tricks","permalink":"/2025/04/04/training-tricks"}},"content":"Notes about GANs from vanilla GAN to CycleGAN and StyleGAN.\\n\\n\x3c!-- truncate --\x3e\\n\\n## (Vanilla) GAN\\n\\nIn short:\\n- Generator $G$ - tries to generate realistic looking images (maximize $D(G(z))$).\\n- Discriminator $D$ - tries to distinguish between real and generated images (minimize $D(G(z))$ and maximize $D(x)$). Discriminator makes for learned loss function for generator.\\n- The original GAN uses binary cross-entropy loss.\\n- Discriminator has softmax at the end and outputs values between $[0, 1]$ (0 - fake, 1 - real).\\n- Best for Generator is for the Discriminator to output ~0.5, which means that D can\'t tell if an image is real or fake (Nash equilibrium).\\n- Loss isn\'t interpretable because both the generator and discriminator can improve in the same time, so even if G\'s image quality improves, the loss can stay the same.\\n\\n:::note\\nThe original GAN from 2014 used MLP layers instead of Conv layers.\\n:::\\n\\n## DCGAN\\n\\nThe simplest GAN that doesn\'t use FFN layers but Conv layers, takes a noise vector and gradually increases it through TransposeConv2d.\\n- Use conv layers instead of FFN layers.\\n- Use conv with a larger stride instead of pooling.\\n- Use batchnorm.\\n- ReLU in $G$ and LeakyReLU in $D$.\\n- $\\\\beta_1=0.5$ (PyTorch default is $0.9$)\\n\\n## cGAN - Conditional GAN\\n\\nRefers to the general idea of adding class information to the GAN.  In the context of the Generator, this usually involves adding labels to the noise (concatenating two vectors). The Discriminator checks if the image is real or fake, having additional information about the class. (The Discriminator returns a scalar for each image).\\n\\n![image.png](https://www.researchgate.net/publication/378656489/figure/fig4/AS:11431281277856805@1726279533229/Conditional-GAN-CGAN-architecture.png)\\n\\n```python\\n# Adding conditioning in the Generator\\ndef forward(x, y):\\n\\tcond = self.embedding(y)\\n\\tx = torch.cat([x, cond], dim=1) # x.shape = [bs, z_dim], cond.shape = [bs, embed_dim]\\n\\treturn self.backbone(x)\\n\\n# Adding conditioning in the Discriminator\\ndef forward(x, y):\\n\\tout = self.net(x)\\n  cond = self.embed(y)\\n  return (x*y).sum(dim=[1, 2, 3])\\n```\\n\\n:::note\\nThere is also [**Projection Discriminator**](https://paperswithcode.com/method/projection-discriminator) as a better conditioning method.\\n:::\\n\\n## acGAN (Auxiliary Classifier GAN)\\n\\nIn acGAN, the discriminator has two outputs: one for real/fake (as in cGAN) and one for classifying the class. The Discriminator doesn\'t receive classes as input because it has to predict them.\\n\\n```python\\n# Discriminator (Generator remains the same as in cGAN)\\ndef forward(self, img):\\n  features = self.model(img)\\n  validity = self.real_fake_head(features)\\n  class_logits = self.class_head(features)\\n  return validity, class_logits\\n  \\nloss_fn_bce = nn.BCELoss()  # For real/fake\\nloss_fn_ce = nn.CrossEntropyLoss()  # For classification\\n\\nfor i in range(epochs):\\n\\t# ...\\n\\t# Discriminator\\n\\td_loss_real = loss_fn_bce(real_validity, torch.ones_like(real_validity)) + \\\\\\\\\\n\\t              loss_fn_ce(real_class_logits, real_labels)\\n\\td_loss_fake = loss_fn_bce(fake_validity, torch.zeros_like(fake_validity)) + \\\\\\\\\\n\\t              loss_fn_ce(fake_class_logits, labels)\\n\\td_loss = d_loss_real + d_loss_fake\\n\\t# ...\\n\\t# Generator\\n\\tg_loss_validity = loss_fn_bce(fake_validity, torch.ones_like(fake_validity))\\n\\tg_loss_class = loss_fn_ce(fake_class_logits, labels)\\n\\tg_loss = g_loss_validity + g_loss_class\\n  # ...\\n```\\n\\n## WGAN & WGAN-GP (Wasserstein GAN with Gradient Penalty)\\n\\nChange of JS divergence (from Vanilla GAN) to Earth Mover Distance (aka Wasserstein distance).\\n\\n- JS divergence works badly if two distributions have small or no overlap (mode collapse, vanishing grads).\\n- Wasserstein distance measures the \\"cost\\" of transforming one distribution into another, works even if there is no overlap between distributions, and gives meaningful values (decreasing loss of the discriminator says that Generator improves in quality).\\n\\nIn the original WGAN (without GP), **weights** are clipped (not the gradient) to ensure Lipschitz continuity.  The parameter $c$ decides how strong the clipping is.\\n\\nThe purpose of the gradient penalty is to keep the critic in the space of 1-Lipschitz functions, so that the critic provides high-quality gradients and the Wasserstein distance is estimated better. (GP only for the critic).\\n\\n## LSGAN (Least Squares GAN)\\n\\nUsing **L2 loss** instead of **BCE loss** to calculate the loss:\\n\\n$$\\n\\\\begin{align*} & loss_{fake} = (D(G(z)) - 0)^2 \\\\\\\\ & loss_{real} = (D(x) - 1)^2 \\\\end{align*}\\n$$\\n\\n## Progressive Growing GAN\\n\\nThe Generator is gradually trained, first at a low resolution, and a new layer that increases the resolution is added periodically. Advantages:\\n- Training stability: Starting with small images causes the model to learn the most important features of the image (shapes, colors) at the beginning, rather than textures and background details. This reduces the risk of training derailment.\\n- Training speed: Training on small images is much cheaper, and the model quickly adapts to a higher resolution.\\n- Better image quality.\\n\\n## Pix2Pix\\n\\nAn architecture created for img2img translation (e.g., sketch \u2192 image). The most important features are the use of **adversarial loss + L1 loss** (they didn\'t use L2 because of lower outlier resistance) and the introduction of **PatchGAN** (image discrimination with tiles instead of a scalar for the entire image - more detailed/local feedback for G).\\n\\n**Loss**\\n\\n- L1 for adhering to the outline but blurred (low-freq structure).\\n- Adv. loss is responsible for sharp edges (high-freq structure).\\n\\n**Model structure and other details**\\n\\n- UNet for the generator.\\n- PatchGAN - discrimination with tiles.\\n- Block structure: Conv + BatchNorm + ReLU.\\n- Dropout as a form of result diversity (**enabled** during inference; no noise vector as a source of randomness and diverse outputs).\\n- LR for D is 2x smaller than for G.\\n- Adam with b1=0.5 and b2=0.999.\\n\\n## CycleGAN\\n\\nThe CycleGAN architecture was made for style-transfer without needing special data for mapping $(A \\\\rightarrow B)$.  To achieve this, it uses a combination of 3 losses:\\n\\n1. Cycle-consistency loss\\n\\nTransforms an image into something and back, and compares the difference between the original and the generated one (the circle/cycle closes).\\n\\n$$\\n\\\\begin{align*} &A \\\\to G_1(A) \\\\to B\' \\\\to G_2(B\') \\\\to A\' \\\\\\\\ &\\\\text{CycleLoss}_1 = ||A - A\'|| \\\\\\\\ \\\\\\\\ &B \\\\to G_2(B) \\\\to A\' \\\\to G_1(A\') \\\\to B\' \\\\\\\\ &\\\\text{CycleLoss}_2 = ||B - B\'|| \\\\\\\\ \\\\\\\\ &\\\\text{CycleLoss}_{total} = \\\\text{CycleLoss}_1 + \\\\text{CycleLoss}_2 \\\\end{align*}\\n$$\\n\\n2. Adversarial loss\\n\\nThe usual loss from the discriminator (each G has its own D).\\n\\n3. Identity loss\\n\\nThe Generator sometimes receives an image that it should generate, so it shouldn\'t change anything in it, just return it (a form of regularization).\\n\\n**Features**\\n\\n- 2 generators and 2 discriminators.\\n- Doesn\'t need a dataset consisting of image pairs (it can be 2 arbitrary datasets from different domains, e.g., photos and paintings).\\n- Generators as **autoencoders, not UNets (unlike Pix2Pix)**.\\n- Uses [PatchGAN from Pix2Pix](https://www.notion.so/GAN-DCGAN-WGAN-gradient-penalty-CycleGAN-14e885efc8fe806998d6f2a6666cb1d2?pvs=21) in the generator.\\n\\n## R3GAN\\n\\n**Summary**\\n\\nAuthors developed a new loss function that has mathematical guarantees of convergence and is stable. With stable training achieved, they modify the architecture (starting from StyleGAN2), remove all the now-superfluous tricks, and modernize the architecture. Ultimately, they arrive at a stable and simpler GAN architecture that rivals previous GANs and diffusion-based SOTA models.\\n\\n**Details**\\n\\n1.  **Re-formulated Loss:** Instead of calculating losses for D and G separately, they are combined.\\n\\n    *   Traditional GAN Loss:\\n\\n        $L(\\\\theta, \\\\psi) = \\\\mathbb{E}_{z \\\\sim p_z} \\\\left[ f \\\\left( D_\\\\psi(G_\\\\theta(z)) \\\\right) \\\\right] + \\\\mathbb{E}_{x \\\\sim p_D} \\\\left[ f \\\\left( -D_\\\\psi(x) \\\\right) \\\\right]$  **G and D are calculated separately**, causing real and fake samples to be far apart (D pushes them apart).\\n\\n    *   \\"Relativistic\\" GAN Loss:\\n\\n        $L(\\\\theta, \\\\psi) = \\\\mathbb{E}_{z \\\\sim p_z, \\\\ x \\\\sim p_D} \\\\left[ f \\\\left( D_\\\\psi(G_\\\\theta(z)) - D_\\\\psi(x) \\\\right) \\\\right]$  Here, **G and D are linked**. D evaluates the \\"realness\\" **relative** to real samples, so fake samples should always be in the vicinity of real samples.\\n\\n2.  **Addition of Zero-Centered Gradient Penalties (R1 and R2):**\\n\\n    *   R1: Penalizes the gradient magnitudes of D on real samples.\\n    *   R2: Penalizes the gradient magnitudes of D on fake samples.\\n\\n    The goal is to reduce the gradients for G when it\'s close to generating real samples (eliminating training oscillations). When $p_{fake} = p_{real}$, the gradients from D should be 0.\\n\\n3.  **Simplification and Modernization of the Architecture:** They start with StyleGAN2 and remove everything that was added to increase stability and convergence. Then, they change the architecture to ResNeXt, but without normalization, because they use [Fixup initialization](https://arxiv.org/abs/1901.09321), which doesn\'t require it.\\n\\n\\n## StyleGAN\\n\\nA complex architecture that has many tricks to achieve stable training and good quality. Allows for interpolation between any two images. [Many versions of StyleGAN have been released](https://blog.paperspace.com/evolution-of-stylegan).  In a nutshell:\\n\\n1. The $z$ noise vector is transformed by the Mapping network into the vector $w$ (latent code).\\n2. The Synthesis network starts creating an image from a learnable tensor 4x4 (Const 4x4x512).\\n3. The gradually upsampled feature map passes through subsequent blocks into which the latent code $w$ (through AdaIN - a type of normalization) and the noise vector $B$ are injected.\\n\\n![image.png](https://machinelearningmastery.com/wp-content/uploads/2019/06/Summary-of-the-StyleGAN-Generator-Model-Architecture.png)\\n\\nInjecting noise through $B$ adds high-frequency details to the image (left with noise, right without noise).\\n\\n![image.png](https://viso.ai/wp-content/uploads/2024/07/effect-noise-adding.jpg)\\n\\n## Other stuff (tips, tricks, minor papers)\\n\\n### Gated Shortcut (arXiv:2201.11351)\\n\\nThe Gated shortcut decides what remains from the residual stream and what is added from the features. **Used only in the Generator.**\\n$f_i$ - input\\n$f_c$ - feature (what\'s on the output of conv, bn, relu)\\n$f_o$ - output\\n$*$ - conv, $\\\\odot$ - concat, $\\\\otimes$ - element-wise multiplication\\n1.  $f_g = \\\\text{sigmoid}(\\\\text{conv}(f_c \\\\odot f_i))$ - _gate_, value 0-1\\n2.  $f_r = \\\\text{conv}(f_c \\\\odot f_i)$ - _refinement_, learned transformation of combined features; what to add\\n3.  $f_o = \\\\text{conv}[f_g \\\\otimes f_c + (1-f_g) \\\\otimes f_r]$ here:\\n    $f_g \\\\otimes f_c$ means how much to take from the feature based on $f_i$ and $f_c$.\\n    $(1-f_g) \\\\otimes f_r$ means the rest, which comes from the combination of $f_i$ and $f_c$.\\n\\nAuthors claims that it\'s better than other gated residuals because it can better add/remove data from the residual stream/features ($f_{EGS}$ is other way to implement a gated shortcut):\\n$$f_{EGS} = f_g \\\\otimes f_i + (1-f_g) \\\\otimes f_c$$\\n> In Eq. 6, $f_{EGS}$ can be interpreted as the weighted summation of the $f_i$ and $f_c$, where weight value is $f_g$. Thus, if $f_g$ is 0.5, it is equal to the scaled-identity shortcut; it cannot effectively keep (or remove) the relevant (or irrelevant) information in $f_c$. In contrast, instead of directly summing $f_i$, the proposed method produces the refinement feature, i.e. $f_r$.\\n\\nThis can probably be used in other architectures. The gating mechanism doesn\'t have to be useful only for GANs (e.g., LSTM).\\n\\n### Checkerboard Artifacts\\n\\nTransposed conv with overlapping stride can cause [**checkerboard artifacts**](https://distill.pub/2016/deconv-checkerboard/) - solution: use regular `Conv2d` + `Upsampling2d(\\"nearest\\")`; then conv can overlap."}]}}')}}]);