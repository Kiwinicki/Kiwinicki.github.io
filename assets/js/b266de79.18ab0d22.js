"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/03/04/gans","metadata":{"permalink":"/2025/03/04/gans","source":"@site/blog/2025-03-04-gans.md","title":"GANs","description":"Notes about GANs from vanilla GAN to CycleGAN and StyleGAN.","date":"2025-03-04T00:00:00.000Z","tags":[],"readingTime":8.415,"hasTruncateMarker":true,"authors":[],"frontMatter":{},"unlisted":false},"content":"Notes about GANs from vanilla GAN to CycleGAN and StyleGAN.\\n\\n\x3c!-- truncate --\x3e\\n\\n## (Vanilla) GAN\\n\\nIn short:\\n- Generator $G$ - tries to generate realistic looking images (maximize $D(G(z))$).\\n- Discriminator $D$ - tries to distinguish between real and generated images (minimize $D(G(z))$ and maximize $D(x)$). Discriminator makes for learned loss function for generator.\\n- The original GAN uses binary cross-entropy loss.\\n- Discriminator has softmax at the end and outputs values between $[0, 1]$ (0 - fake, 1 - real).\\n- Best for Generator is for the Discriminator to output ~0.5, which means that D can\'t tell if an image is real or fake (Nash equilibrium).\\n- Loss isn\'t interpretable because both the generator and discriminator can improve in the same time, so even if G\'s image quality improves, the loss can stay the same.\\n\\n:::note\\nThe original GAN from 2014 used MLP layers instead of Conv layers.\\n:::\\n\\n## DCGAN\\n\\nThe simplest GAN that doesn\'t use FFN layers but Conv layers, takes a noise vector and gradually increases it through TransposeConv2d.\\n- Use conv layers instead of FFN layers.\\n- Use conv with a larger stride instead of pooling.\\n- Use batchnorm.\\n- ReLU in $G$ and LeakyReLU in $D$.\\n- $\\\\beta_1=0.5$ (PyTorch default is $0.9$)\\n\\n## cGAN - Conditional GAN\\n\\nRefers to the general idea of adding class information to the GAN.  In the context of the Generator, this usually involves adding labels to the noise (concatenating two vectors). The Discriminator checks if the image is real or fake, having additional information about the class. (The Discriminator returns a scalar for each image).\\n\\n![image.png](https://www.researchgate.net/publication/378656489/figure/fig4/AS:11431281277856805@1726279533229/Conditional-GAN-CGAN-architecture.png)\\n\\n```python\\n# Adding conditioning in the Generator\\ndef forward(x, y):\\n\\tcond = self.embedding(y)\\n\\tx = torch.cat([x, cond], dim=1) # x.shape = [bs, z_dim], cond.shape = [bs, embed_dim]\\n\\treturn self.backbone(x)\\n\\n# Adding conditioning in the Discriminator\\ndef forward(x, y):\\n\\tout = self.net(x)\\n  cond = self.embed(y)\\n  return (x*y).sum(dim=[1, 2, 3])\\n```\\n\\n:::note\\nThere is also [**Projection Discriminator**](https://paperswithcode.com/method/projection-discriminator) as a better conditioning method.\\n:::\\n\\n## acGAN (Auxiliary Classifier GAN)\\n\\nIn acGAN, the discriminator has two outputs: one for real/fake (as in cGAN) and one for classifying the class. The Discriminator doesn\'t receive classes as input because it has to predict them.\\n\\n```python\\n# Discriminator (Generator remains the same as in cGAN)\\ndef forward(self, img):\\n  features = self.model(img)\\n  validity = self.real_fake_head(features)\\n  class_logits = self.class_head(features)\\n  return validity, class_logits\\n  \\nloss_fn_bce = nn.BCELoss()  # For real/fake\\nloss_fn_ce = nn.CrossEntropyLoss()  # For classification\\n\\nfor i in range(epochs):\\n\\t# ...\\n\\t# Discriminator\\n\\td_loss_real = loss_fn_bce(real_validity, torch.ones_like(real_validity)) + \\\\\\\\\\n\\t              loss_fn_ce(real_class_logits, real_labels)\\n\\td_loss_fake = loss_fn_bce(fake_validity, torch.zeros_like(fake_validity)) + \\\\\\\\\\n\\t              loss_fn_ce(fake_class_logits, labels)\\n\\td_loss = d_loss_real + d_loss_fake\\n\\t# ...\\n\\t# Generator\\n\\tg_loss_validity = loss_fn_bce(fake_validity, torch.ones_like(fake_validity))\\n\\tg_loss_class = loss_fn_ce(fake_class_logits, labels)\\n\\tg_loss = g_loss_validity + g_loss_class\\n  # ...\\n```\\n\\n## WGAN & WGAN-GP (Wasserstein GAN with Gradient Penalty)\\n\\nChange of JS divergence (from Vanilla GAN) to Earth Mover Distance (aka Wasserstein distance).\\n\\n- JS divergence works badly if two distributions have small or no overlap (mode collapse, vanishing grads).\\n- Wasserstein distance measures the \\"cost\\" of transforming one distribution into another, works even if there is no overlap between distributions, and gives meaningful values (decreasing loss of the discriminator says that Generator improves in quality).\\n\\nIn the original WGAN (without GP), **weights** are clipped (not the gradient) to ensure Lipschitz continuity.  The parameter $c$ decides how strong the clipping is.\\n\\nThe purpose of the gradient penalty is to keep the critic in the space of 1-Lipschitz functions, so that the critic provides high-quality gradients and the Wasserstein distance is estimated better. (GP only for the critic).\\n\\n## LSGAN (Least Squares GAN)\\n\\nUsing **L2 loss** instead of **BCE loss** to calculate the loss:\\n\\n$$\\n\\\\begin{align*} & loss_{fake} = (D(G(z)) - 0)^2 \\\\\\\\ & loss_{real} = (D(x) - 1)^2 \\\\end{align*}\\n$$\\n\\n## Progressive Growing GAN\\n\\nThe Generator is gradually trained, first at a low resolution, and a new layer that increases the resolution is added periodically. Advantages:\\n- Training stability: Starting with small images causes the model to learn the most important features of the image (shapes, colors) at the beginning, rather than textures and background details. This reduces the risk of training derailment.\\n- Training speed: Training on small images is much cheaper, and the model quickly adapts to a higher resolution.\\n- Better image quality.\\n\\n## Pix2Pix\\n\\nAn architecture created for img2img translation (e.g., sketch \u2192 image). The most important features are the use of **adversarial loss + L1 loss** (they didn\'t use L2 because of lower outlier resistance) and the introduction of **PatchGAN** (image discrimination with tiles instead of a scalar for the entire image - more detailed/local feedback for G).\\n\\n**Loss**\\n\\n- L1 for adhering to the outline but blurred (low-freq structure).\\n- Adv. loss is responsible for sharp edges (high-freq structure).\\n\\n**Model structure and other details**\\n\\n- UNet for the generator.\\n- PatchGAN - discrimination with tiles.\\n- Block structure: Conv + BatchNorm + ReLU.\\n- Dropout as a form of result diversity (**enabled** during inference; no noise vector as a source of randomness and diverse outputs).\\n- LR for D is 2x smaller than for G.\\n- Adam with b1=0.5 and b2=0.999.\\n\\n## CycleGAN\\n\\nThe CycleGAN architecture was made for style-transfer without needing special data for mapping $(A \\\\rightarrow B)$.  To achieve this, it uses a combination of 3 losses:\\n\\n1. Cycle-consistency loss\\n\\nTransforms an image into something and back, and compares the difference between the original and the generated one (the circle/cycle closes).\\n\\n$$\\n\\\\begin{align*} &A \\\\to G_1(A) \\\\to B\' \\\\to G_2(B\') \\\\to A\' \\\\\\\\ &\\\\text{CycleLoss}_1 = ||A - A\'|| \\\\\\\\ \\\\\\\\ &B \\\\to G_2(B) \\\\to A\' \\\\to G_1(A\') \\\\to B\' \\\\\\\\ &\\\\text{CycleLoss}_2 = ||B - B\'|| \\\\\\\\ \\\\\\\\ &\\\\text{CycleLoss}_{total} = \\\\text{CycleLoss}_1 + \\\\text{CycleLoss}_2 \\\\end{align*}\\n$$\\n\\n2. Adversarial loss\\n\\nThe usual loss from the discriminator (each G has its own D).\\n\\n3. Identity loss\\n\\nThe Generator sometimes receives an image that it should generate, so it shouldn\'t change anything in it, just return it (a form of regularization).\\n\\n**Features**\\n\\n- 2 generators and 2 discriminators.\\n- Doesn\'t need a dataset consisting of image pairs (it can be 2 arbitrary datasets from different domains, e.g., photos and paintings).\\n- Generators as **autoencoders, not UNets (unlike Pix2Pix)**.\\n- Uses [PatchGAN from Pix2Pix](https://www.notion.so/GAN-DCGAN-WGAN-gradient-penalty-CycleGAN-14e885efc8fe806998d6f2a6666cb1d2?pvs=21) in the generator.\\n\\n## R3GAN\\n\\n**Summary**\\n\\nAuthors developed a new loss function that has mathematical guarantees of convergence and is stable. With stable training achieved, they modify the architecture (starting from StyleGAN2), remove all the now-superfluous tricks, and modernize the architecture. Ultimately, they arrive at a stable and simpler GAN architecture that rivals previous GANs and diffusion-based SOTA models.\\n\\n**Details**\\n\\n1.  **Re-formulated Loss:** Instead of calculating losses for D and G separately, they are combined.\\n\\n    *   Traditional GAN Loss:\\n\\n        $L(\\\\theta, \\\\psi) = \\\\mathbb{E}_{z \\\\sim p_z} \\\\left[ f \\\\left( D_\\\\psi(G_\\\\theta(z)) \\\\right) \\\\right] + \\\\mathbb{E}_{x \\\\sim p_D} \\\\left[ f \\\\left( -D_\\\\psi(x) \\\\right) \\\\right]$  **G and D are calculated separately**, causing real and fake samples to be far apart (D pushes them apart).\\n\\n    *   \\"Relativistic\\" GAN Loss:\\n\\n        $L(\\\\theta, \\\\psi) = \\\\mathbb{E}_{z \\\\sim p_z, \\\\ x \\\\sim p_D} \\\\left[ f \\\\left( D_\\\\psi(G_\\\\theta(z)) - D_\\\\psi(x) \\\\right) \\\\right]$  Here, **G and D are linked**. D evaluates the \\"realness\\" **relative** to real samples, so fake samples should always be in the vicinity of real samples.\\n\\n2.  **Addition of Zero-Centered Gradient Penalties (R1 and R2):**\\n\\n    *   R1: Penalizes the gradient magnitudes of D on real samples.\\n    *   R2: Penalizes the gradient magnitudes of D on fake samples.\\n\\n    The goal is to reduce the gradients for G when it\'s close to generating real samples (eliminating training oscillations). When $p_{fake} = p_{real}$, the gradients from D should be 0.\\n\\n3.  **Simplification and Modernization of the Architecture:** They start with StyleGAN2 and remove everything that was added to increase stability and convergence. Then, they change the architecture to ResNeXt, but without normalization, because they use [Fixup initialization](https://arxiv.org/abs/1901.09321), which doesn\'t require it.\\n\\n\\n## StyleGAN\\n\\nA complex architecture that has many tricks to achieve stable training and good quality. Allows for interpolation between any two images. [Many versions of StyleGAN have been released](https://blog.paperspace.com/evolution-of-stylegan).  In a nutshell:\\n\\n1. The $z$ noise vector is transformed by the Mapping network into the vector $w$ (latent code).\\n2. The Synthesis network starts creating an image from a learnable tensor 4x4 (Const 4x4x512).\\n3. The gradually upsampled feature map passes through subsequent blocks into which the latent code $w$ (through AdaIN - a type of normalization) and the noise vector $B$ are injected.\\n\\n![image.png](https://machinelearningmastery.com/wp-content/uploads/2019/06/Summary-of-the-StyleGAN-Generator-Model-Architecture.png)\\n\\nInjecting noise through $B$ adds high-frequency details to the image (left with noise, right without noise).\\n\\n![image.png](https://viso.ai/wp-content/uploads/2024/07/effect-noise-adding.jpg)\\n\\n## Other stuff (tips, tricks, minor papers)\\n\\n### Gated Shortcut (arXiv:2201.11351)\\n\\nThe Gated shortcut decides what remains from the residual stream and what is added from the features. **Used only in the Generator.**\\n$f_i$ - input\\n$f_c$ - feature (what\'s on the output of conv, bn, relu)\\n$f_o$ - output\\n$*$ - conv, $\\\\odot$ - concat, $\\\\otimes$ - element-wise multiplication\\n1.  $f_g = \\\\text{sigmoid}(\\\\text{conv}(f_c \\\\odot f_i))$ - _gate_, value 0-1\\n2.  $f_r = \\\\text{conv}(f_c \\\\odot f_i)$ - _refinement_, learned transformation of combined features; what to add\\n3.  $f_o = \\\\text{conv}[f_g \\\\otimes f_c + (1-f_g) \\\\otimes f_r]$ here:\\n    $f_g \\\\otimes f_c$ means how much to take from the feature based on $f_i$ and $f_c$.\\n    $(1-f_g) \\\\otimes f_r$ means the rest, which comes from the combination of $f_i$ and $f_c$.\\n\\nAuthors claims that it\'s better than other gated residuals because it can better add/remove data from the residual stream/features ($f_{EGS}$ is other way to implement a gated shortcut):\\n$$f_{EGS} = f_g \\\\otimes f_i + (1-f_g) \\\\otimes f_c$$\\n> In Eq. 6, $f_{EGS}$ can be interpreted as the weighted summation of the $f_i$ and $f_c$, where weight value is $f_g$. Thus, if $f_g$ is 0.5, it is equal to the scaled-identity shortcut; it cannot effectively keep (or remove) the relevant (or irrelevant) information in $f_c$. In contrast, instead of directly summing $f_i$, the proposed method produces the refinement feature, i.e. $f_r$.\\n\\nThis can probably be used in other architectures. The gating mechanism doesn\'t have to be useful only for GANs (e.g., LSTM).\\n\\n### Checkerboard Artifacts\\n\\nTransposed conv with overlapping stride can cause [**checkerboard artifacts**](https://distill.pub/2016/deconv-checkerboard/) - solution: use regular `Conv2d` + `Upsampling2d(\\"nearest\\")`; then conv can overlap."}]}}')}}]);